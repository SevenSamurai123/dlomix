{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f91f5c",
   "metadata": {},
   "source": [
    "*Experimental*\n",
    "__________\n",
    "\n",
    "## Example Intesity Pipeline with PTMs using a PROSPECT POOL (Json Input) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b0572",
   "metadata": {},
   "source": [
    "### Get the dataset from PROSPECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop branch of PROSPECT\n",
    "!pip install git+https://github.com/wilhelm-lab/PROSPECT.git@develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/wilhelm-lab/dlomix.git@develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prospectdataset as prospect \n",
    "data_dir = \"./data\"\n",
    "pool_keyword = \"third\"\n",
    "prospect.download_dataset(\"all\", data_dir, pool_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# pick the path of the metadata file, can also be simply copied and pasted from previous cell outout \n",
    "#meta_data_filepath = './data/TUM_third_pool_meta_data.parquet'\n",
    "\n",
    "meta_data_filepath = glob.glob(os.path.join(data_dir, \"*meta_data*.parquet\"))[0]\n",
    "meta_data_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation file names and paths\n",
    "\n",
    "pool_folder_path = os.path.splitext(glob.glob(os.path.join(data_dir, \"*.zip\"))[0])[0]\n",
    "\n",
    "annotations_filepaths = glob.glob(os.path.join(pool_folder_path, \"*.parquet\"))\n",
    "annotations_names = [Path(f).stem for f in annotations_filepath]\n",
    "\n",
    "annotations_names, annotations_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a7073",
   "metadata": {},
   "source": [
    "### Prepare input data dict for DLOmix Dataset Class (JSON mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01785c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_data_dict = {\n",
    "    \"metadata\": meta_data_filepath,\n",
    "    \"annotations\": {\n",
    "        pool_keyword: dict(zip(annotations_names, annotations_filepaths))\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"target_column_key\": \"intensities_raw\"\n",
    "    }\n",
    "}\n",
    "\n",
    "input_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d658d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# later we can feed the dict directly as a data source, for now we stick to json format\n",
    "\n",
    "import json\n",
    "with open(\"input_config.json\", 'w') as fp:\n",
    "    json.dump(input_data_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ed460",
   "metadata": {},
   "source": [
    "### Create Intensity Dataset from the downloaded Pool using features and parser\n",
    "This would take a couple of minutes since it:\n",
    "- reads the metadata and the annotation files\n",
    "- does some filtering and wrangling of the data\n",
    "- produces the final input data for intensity\n",
    "- extracts the features and prepares the TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8999d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data import IntensityDataset\n",
    "from dlomix.data.feature_extractors import (\n",
    "    ModificationGainFeature,\n",
    "    ModificationLocationFeature,\n",
    "    ModificationLossFeature,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LENGTH = 30\n",
    "\n",
    "int_data = IntensityDataset(\n",
    "    data_source=\"input_config.json\",\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    val_ratio=0.15,\n",
    "    precursor_charge_col=\"precursor_charge_onehot\",\n",
    "    sequence_col=\"modified_sequence\",\n",
    "    collision_energy_col=\"collision_energy_aligned_normed\",\n",
    "    intensities_col=\"intensities_raw\",\n",
    "    features_to_extract=[\n",
    "        ModificationLocationFeature(),\n",
    "        ModificationLossFeature(),\n",
    "        ModificationGainFeature(),\n",
    "    ],\n",
    "    parser=\"proforma\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Training examples\", BATCH_SIZE * len(int_data.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Validation examples\", BATCH_SIZE * len(int_data.val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547b21d",
   "metadata": {},
   "source": [
    "### Create Model and compile it with the respective loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eefc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from dlomix.models import PrositIntensityPredictor\n",
    "from dlomix.losses import masked_spectral_distance\n",
    "\n",
    "model = PrositIntensityPredictor(seq_length=30, use_ptm_counts=True)\n",
    "\n",
    "# create the optimizer object\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# compile the model  with the optimizer and the metrics we want to use, we can add our custom metric\n",
    "model.compile(optimizer=optimizer, loss=masked_spectral_distance, metrics=[\"mae\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeec244",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(int_data.train_data,\n",
    "                    validation_data=int_data.val_data,\n",
    "                    epochs=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
